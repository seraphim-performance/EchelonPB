{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYioTB0j5-EJ",
        "outputId": "930aaaef-8cfc-4720-8397-8b843c966628"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.4.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.34.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWpOYmF87egQ"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/contract-nli.zip /home/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3Dxkh1X7mZj",
        "outputId": "258683d7-8fc1-481b-8045-b3a91b913f43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /home/contract-nli.zip\n",
            "replace /home/contract-nli/dev.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!unzip /home/contract-nli.zip -d /home/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVcyI7icwMHm"
      },
      "outputs": [],
      "source": [
        "#Load libraries\n",
        "import torch\n",
        "from transformers import get_scheduler, AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, AutoModel, AutoProcessor\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import accelerate\n",
        "from PIL import Image\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "#from datasets import Dataset\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image as Im\n",
        "from IPython.display import display\n",
        "from torch.hub import load\n",
        "from torch.optim import AdamW\n",
        "from pathlib import Path\n",
        "import os\n",
        "import requests\n",
        "import gc\n",
        "from peft import PeftModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "25b7ee0cdee84e6c9952f7cf0c63dd27",
            "afc6720498714f5aa5675e24956ace69",
            "f0c0594f039243999e2caf1a5cbf096e",
            "1b6efa5f487f44bc9af2405c33157db8",
            "986d32c09d254912b7d3c8a039273284",
            "a19c03e0988b480590bdeaa2c8e592f4",
            "7dc0af7657e44005b6c96e8651924096",
            "8ceafc43f0544ba09c02f39c5ed93ba4",
            "1c74f736ee914ed29419618bd54791ac",
            "1f80285b34b8401ca6ae3724107e4e9a",
            "b26733244f7a4e7db205d43bd5e871a1"
          ]
        },
        "id": "8U9Y5gys8Ol7",
        "outputId": "a051dfb0-e7d6-4569-b91e-357e358b72de"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25b7ee0cdee84e6c9952f7cf0c63dd27",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.3\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.3\", torch_dtype=torch.float16)\n",
        "\n",
        "# Prepare optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=5e-6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sY9HwBo6KJf3"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8OR0jcZI5UN",
        "outputId": "fd8f3d40-809a-4caa-b820-8ea2b2d9cb56"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PeftModel(\n",
              "  (base_model): LoraModel(\n",
              "    (model): MistralForCausalLM(\n",
              "      (model): MistralModel(\n",
              "        (embed_tokens): Embedding(32768, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x MistralDecoderLayer(\n",
              "            (self_attn): MistralSdpaAttention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (rotary_emb): MistralRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): MistralMLP(\n",
              "              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define LoRA config\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # The rank of the decomposition\n",
        "    lora_alpha=32,  # Controls the scaling of the LoRA weights\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # Target projection matrices in transformer layers\n",
        "    lora_dropout=0.1,  # Dropout for LoRA layers\n",
        "    bias=\"none\",  # Don't add bias\n",
        ")\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "at__dt0y1etE"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the custom Dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, directory, tokenizer, max_length=8192, stride=4096):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.directory = directory\n",
        "        self.stride = stride\n",
        "        self.token_chunks = self._load_and_tokenize()\n",
        "\n",
        "    def _load_and_tokenize(self):\n",
        "        \"\"\"\n",
        "        Load all text files from the directory and tokenize them with sliding window chunks.\n",
        "        \"\"\"\n",
        "        token_chunks = []\n",
        "        # Loop through all text files in the directory\n",
        "        for filename in os.listdir(self.directory):\n",
        "            if filename.endswith(\".txt\"):\n",
        "                file_path = os.path.join(self.directory, filename)\n",
        "\n",
        "                # Read the text file\n",
        "                with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                    text = file.read()\n",
        "\n",
        "                # Tokenize and chunk with sliding window\n",
        "                chunks = self._sliding_window_tokenization(text)\n",
        "                token_chunks.extend(chunks)  # Append chunks from this file\n",
        "        return token_chunks\n",
        "\n",
        "    def _sliding_window_tokenization(self, text):\n",
        "        \"\"\"\n",
        "        Tokenizes the text and creates sliding window chunks.\n",
        "        \"\"\"\n",
        "        # Tokenize the text without truncation, no special tokens added\n",
        "        tokenized = self.tokenizer(text, return_tensors=\"pt\", truncation=False, add_special_tokens=False)\n",
        "        tokens = tokenized['input_ids'].squeeze(0)\n",
        "\n",
        "        total_tokens = tokens.size(0)\n",
        "        chunks = []\n",
        "\n",
        "        # Create sliding window chunks\n",
        "        for i in range(0, total_tokens, self.stride):\n",
        "            window = tokens[i:i + self.max_length]\n",
        "            attention_mask = torch.ones_like(window)\n",
        "\n",
        "            if len(window) < self.max_length:\n",
        "                # Pad the window and attention mask\n",
        "                attention_mask = torch.cat([attention_mask, torch.zeros(self.max_length - len(window), dtype=torch.long)], dim=0)\n",
        "                window = torch.cat([window, torch.zeros(self.max_length - len(window), dtype=torch.long)], dim=0)\n",
        "\n",
        "            chunks.append((window, attention_mask))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.token_chunks)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_ids, attention_mask = self.token_chunks[idx]\n",
        "        return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
        "\n",
        "# Function to load text files into a dataset and prepare them for Mistral v0.3\n",
        "def load_texts_to_dataset(directory, tokenizer, batch_size=8, max_length=8192):\n",
        "    stride = max_length // 2  # 50% overlap\n",
        "    # Create the custom dataset\n",
        "    dataset = TextDataset(directory, tokenizer, max_length=max_length, stride=stride)\n",
        "\n",
        "    # Create a DataLoader for batching\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "# Iterate through the dataloader\n",
        "#for batch in dataloader:\n",
        "#    input_ids = batch['input_ids']\n",
        "#    attention_mask = batch['attention_mask']\n",
        "#    print(\"Input IDs shape:\", input_ids.shape)\n",
        "#    print(\"Attention mask shape:\", attention_mask.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tq8x5WZPBbFV"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Setting up config\n",
        "directory = '/home/contract-nli/raw'\n",
        "dataloader = load_texts_to_dataset(directory, tokenizer, batch_size=1, max_length=2048)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_max_length=2048"
      ],
      "metadata": {
        "id": "uN2vL78NU2_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSGiXPe5JJOO",
        "outputId": "8ebe0737-8ab9-4422-a01c-d0f09862e096"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 1/100 completed with loss: 8.931796073913574\n",
            "Epoch 1/100 took 703.2538 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 2/100 completed with loss: 1.2718234062194824\n",
            "Epoch 2/100 took 704.3287 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 3/100 completed with loss: 10.918999671936035\n",
            "Epoch 3/100 took 702.0434 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 4/100 completed with loss: 14.241212844848633\n",
            "Epoch 4/100 took 701.8674 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 5/100 completed with loss: 2.7161953449249268\n",
            "Epoch 5/100 took 701.8207 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 6/100 completed with loss: 16.24726676940918\n",
            "Epoch 6/100 took 701.9738 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 7/100 completed with loss: 7.738148212432861\n",
            "Epoch 7/100 took 701.9979 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 8/100 completed with loss: 1.2062828540802002\n",
            "Epoch 8/100 took 701.9944 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 9/100 completed with loss: 10.342475891113281\n",
            "Epoch 9/100 took 702.0506 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 10/100 completed with loss: 8.931796073913574\n",
            "Epoch 10/100 took 702.1325 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 11/100 completed with loss: 1.6767457723617554\n",
            "Epoch 11/100 took 702.1563 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 12/100 completed with loss: 1.5762720108032227\n",
            "Epoch 12/100 took 701.7830 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 13/100 completed with loss: 12.8407564163208\n",
            "Epoch 13/100 took 702.1305 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 14/100 completed with loss: 1.196982502937317\n",
            "Epoch 14/100 took 702.2249 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 15/100 completed with loss: 1.360924243927002\n",
            "Epoch 15/100 took 702.2010 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 16/100 completed with loss: 6.245309352874756\n",
            "Epoch 16/100 took 702.1312 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 17/100 completed with loss: 1.1460227966308594\n",
            "Epoch 17/100 took 702.0791 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 18/100 completed with loss: 14.745670318603516\n",
            "Epoch 18/100 took 702.1239 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 19/100 completed with loss: 1.5762720108032227\n",
            "Epoch 19/100 took 701.9922 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 20/100 completed with loss: 1.1291545629501343\n",
            "Epoch 20/100 took 701.9512 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 21/100 completed with loss: 1.0993852615356445\n",
            "Epoch 21/100 took 701.8442 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 22/100 completed with loss: 6.152869701385498\n",
            "Epoch 22/100 took 701.7202 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 23/100 completed with loss: 6.07118034362793\n",
            "Epoch 23/100 took 701.8690 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 24/100 completed with loss: 19.651838302612305\n",
            "Epoch 24/100 took 701.9774 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 25/100 completed with loss: 1.8721972703933716\n",
            "Epoch 25/100 took 701.8969 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 26/100 completed with loss: 11.716333389282227\n",
            "Epoch 26/100 took 701.9124 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 27/100 completed with loss: 12.906726837158203\n",
            "Epoch 27/100 took 701.9017 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 28/100 completed with loss: 19.799652099609375\n",
            "Epoch 28/100 took 701.8739 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 29/100 completed with loss: 1.1750078201293945\n",
            "Epoch 29/100 took 701.8722 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 30/100 completed with loss: 1.1481484174728394\n",
            "Epoch 30/100 took 701.7885 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 31/100 completed with loss: 16.214399337768555\n",
            "Epoch 31/100 took 701.8640 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 32/100 completed with loss: 1.1494048833847046\n",
            "Epoch 32/100 took 701.4505 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 33/100 completed with loss: 1.145720362663269\n",
            "Epoch 33/100 took 701.4032 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 34/100 completed with loss: 1.1094074249267578\n",
            "Epoch 34/100 took 701.3942 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 35/100 completed with loss: 9.37615966796875\n",
            "Epoch 35/100 took 701.4953 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 36/100 completed with loss: 17.499568939208984\n",
            "Epoch 36/100 took 701.5573 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 37/100 completed with loss: 14.085918426513672\n",
            "Epoch 37/100 took 701.4946 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 38/100 completed with loss: 17.44083023071289\n",
            "Epoch 38/100 took 701.4921 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 39/100 completed with loss: 1.388633131980896\n",
            "Epoch 39/100 took 701.4704 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 40/100 completed with loss: 7.043787956237793\n",
            "Epoch 40/100 took 701.6068 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 41/100 completed with loss: 9.058558464050293\n",
            "Epoch 41/100 took 702.3310 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 42/100 completed with loss: 15.033185958862305\n",
            "Epoch 42/100 took 703.5498 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 43/100 completed with loss: 20.859540939331055\n",
            "Epoch 43/100 took 703.3908 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 44/100 completed with loss: 3.729970693588257\n",
            "Epoch 44/100 took 702.7678 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 45/100 completed with loss: 11.447111129760742\n",
            "Epoch 45/100 took 701.7385 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 46/100 completed with loss: 1.283773422241211\n",
            "Epoch 46/100 took 701.8115 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 47/100 completed with loss: 1.360924243927002\n",
            "Epoch 47/100 took 701.9641 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 48/100 completed with loss: 8.75423812866211\n",
            "Epoch 48/100 took 702.0149 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 49/100 completed with loss: 1.1520804166793823\n",
            "Epoch 49/100 took 702.1766 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 50/100 completed with loss: 17.44083023071289\n",
            "Epoch 50/100 took 701.7724 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 51/100 completed with loss: 1.0611401796340942\n",
            "Epoch 51/100 took 701.8988 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 52/100 completed with loss: 7.434463024139404\n",
            "Epoch 52/100 took 701.5858 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 53/100 completed with loss: 1.06938898563385\n",
            "Epoch 53/100 took 701.8946 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 54/100 completed with loss: 13.04321002960205\n",
            "Epoch 54/100 took 701.9310 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 55/100 completed with loss: 16.603452682495117\n",
            "Epoch 55/100 took 701.9342 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 56/100 completed with loss: 5.426334381103516\n",
            "Epoch 56/100 took 702.7013 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 57/100 completed with loss: 8.203343391418457\n",
            "Epoch 57/100 took 702.4942 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 58/100 completed with loss: 1.215550422668457\n",
            "Epoch 58/100 took 702.6181 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 59/100 completed with loss: 6.508048057556152\n",
            "Epoch 59/100 took 702.9435 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 60/100 completed with loss: 1.307478904724121\n",
            "Epoch 60/100 took 702.6323 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 61/100 completed with loss: 2.6368043422698975\n",
            "Epoch 61/100 took 702.2795 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 62/100 completed with loss: 17.103567123413086\n",
            "Epoch 62/100 took 701.9217 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 63/100 completed with loss: 10.18877124786377\n",
            "Epoch 63/100 took 701.9754 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 64/100 completed with loss: 8.045639038085938\n",
            "Epoch 64/100 took 701.6733 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 65/100 completed with loss: 4.239570617675781\n",
            "Epoch 65/100 took 701.6745 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 66/100 completed with loss: 11.301092147827148\n",
            "Epoch 66/100 took 701.8091 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 67/100 completed with loss: 1.0733777284622192\n",
            "Epoch 67/100 took 701.8116 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 68/100 completed with loss: 1.0203453302383423\n",
            "Epoch 68/100 took 701.7231 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 69/100 completed with loss: 14.62094497680664\n",
            "Epoch 69/100 took 701.7466 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 70/100 completed with loss: 19.650474548339844\n",
            "Epoch 70/100 took 701.6970 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 71/100 completed with loss: 11.067955017089844\n",
            "Epoch 71/100 took 701.8927 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 72/100 completed with loss: 7.748190879821777\n",
            "Epoch 72/100 took 701.8352 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 73/100 completed with loss: 10.143403053283691\n",
            "Epoch 73/100 took 702.1082 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 74/100 completed with loss: 18.139150619506836\n",
            "Epoch 74/100 took 702.2675 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 75/100 completed with loss: 1.0531476736068726\n",
            "Epoch 75/100 took 701.9530 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 76/100 completed with loss: 1.373197317123413\n",
            "Epoch 76/100 took 702.3283 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 77/100 completed with loss: 0.930746853351593\n",
            "Epoch 77/100 took 702.0705 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 78/100 completed with loss: 1.0811759233474731\n",
            "Epoch 78/100 took 702.3039 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 79/100 completed with loss: 1.3353374004364014\n",
            "Epoch 79/100 took 702.1929 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 80/100 completed with loss: 1.2327539920806885\n",
            "Epoch 80/100 took 701.9953 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 81/100 completed with loss: 1.373197317123413\n",
            "Epoch 81/100 took 702.2072 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 82/100 completed with loss: 5.890863418579102\n",
            "Epoch 82/100 took 702.1363 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 83/100 completed with loss: 10.87004566192627\n",
            "Epoch 83/100 took 702.1634 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 84/100 completed with loss: 18.660062789916992\n",
            "Epoch 84/100 took 702.0588 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 85/100 completed with loss: 1.395729899406433\n",
            "Epoch 85/100 took 702.1091 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 86/100 completed with loss: 1.3485548496246338\n",
            "Epoch 86/100 took 702.0796 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 87/100 completed with loss: 6.46048641204834\n",
            "Epoch 87/100 took 702.0104 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 88/100 completed with loss: 10.26431941986084\n",
            "Epoch 88/100 took 702.0239 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 89/100 completed with loss: 3.025179386138916\n",
            "Epoch 89/100 took 702.0802 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 90/100 completed with loss: 1.107342004776001\n",
            "Epoch 90/100 took 702.1414 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 91/100 completed with loss: 1.2578818798065186\n",
            "Epoch 91/100 took 701.9953 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 92/100 completed with loss: 17.89056396484375\n",
            "Epoch 92/100 took 702.0323 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 93/100 completed with loss: 20.242292404174805\n",
            "Epoch 93/100 took 702.0666 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 94/100 completed with loss: 16.67806625366211\n",
            "Epoch 94/100 took 702.1269 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 95/100 completed with loss: 1.226657748222351\n",
            "Epoch 95/100 took 702.0955 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 96/100 completed with loss: 12.577385902404785\n",
            "Epoch 96/100 took 702.2377 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 97/100 completed with loss: 1.370510220527649\n",
            "Epoch 97/100 took 706.7854 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 98/100 completed with loss: 17.788135528564453\n",
            "Epoch 98/100 took 706.9049 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 99/100 completed with loss: 11.628151893615723\n",
            "Epoch 99/100 took 706.8599 seconds\n",
            "GPU memory allocated: 14122.81 MB\n",
            "GPU memory reserved: 27876.00 MB\n",
            "Epoch 100/100 completed with loss: 3.3927254676818848\n",
            "Epoch 100/100 took 706.8822 seconds\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 100\n",
        "# Scheduler can be used to manage learning rate decay\n",
        "num_training_steps = num_epochs * len(dataloader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    start_epoch_time = time.time()  # Start timer for the epoch\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Move input tensors to the appropriate device\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "\n",
        "    end_epoch_time = time.time()\n",
        "\n",
        "    allocated_memory = torch.cuda.memory_allocated() / (1024 ** 2)  # Convert to MB\n",
        "    reserved_memory = torch.cuda.memory_reserved() / (1024 ** 2)  # Convert to MB\n",
        "    print(f\"GPU memory allocated: {allocated_memory:.2f} MB\")\n",
        "    print(f\"GPU memory reserved: {reserved_memory:.2f} MB\")\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs} completed with loss: {loss.item()}\")\n",
        "\n",
        "    epoch_time = end_epoch_time - start_epoch_time\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} took {epoch_time:.4f} seconds\")\n",
        "    if epoch % 10 == 0:\n",
        "        savepath = \"/content/drive/MyDrive/mistral-lora-finetuned/\" + str(epoch) + \".pth\"\n",
        "        torch.save({\n",
        "          'model_state_dict': model.state_dict(),\n",
        "          'optimizer_state_dict': optimizer.state_dict(),  # If you have an optimizer\n",
        "          'epoch': epoch  # You can save other metadata like the current epoch\n",
        "        }, savepath)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTIZxMi5T58O",
        "outputId": "89561f12-2723-4292-bd36-97518f90ee8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PW7rYkS6Kesj"
      },
      "outputs": [],
      "source": [
        "\n",
        "savepath = \"/content/drive/MyDrive/mistral-lora-finetuned/\" + str(epoch) + \".pth\"\n",
        "torch.save({\n",
        "  'model_state_dict': model.state_dict(),\n",
        "  'optimizer_state_dict': optimizer.state_dict(),  # If you have an optimizer\n",
        "  'epoch': epoch  # You can save other metadata like the current epoch\n",
        "}, savepath)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/mistralora\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/mistralora\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHHnBi9_bSIR",
        "outputId": "601fb110-2808-439a-fb17-75acda750d3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/mistralora/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/mistralora/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/mistralora/tokenizer.model',\n",
              " '/content/drive/MyDrive/mistralora/added_tokens.json',\n",
              " '/content/drive/MyDrive/mistralora/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_saved_model_base = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.3\", torch_dtype=torch.float16)\n",
        "my_saved_model_tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/mistralora\")\n",
        "my_saved_model = PeftModel.from_pretrained(my_saved_model_base, \"/content/drive/MyDrive/mistralora\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "f74f70f3a911414e8926ebd5d9eaa34b",
            "68405c4024de40db9e729a760814dd2b",
            "86905d641d9f4315a2d9725180e2f5d4",
            "a90e5f7295e64a69990db450a91d091d",
            "0362c807eeb04e24a939e64137493a7a",
            "c7d75989b71e4e0fb2162a118209ef6a",
            "595c1ff947e9424cb6036c84148eced7",
            "11518c1de3d9427aa75bd70f7eadccb6",
            "a45d6423c7b743f39eb993597add1aeb",
            "af27f7f95cc7414dac097daad095377b",
            "5621228c360143368343e2ded7219d40"
          ]
        },
        "id": "m7EQAAkbbukn",
        "outputId": "4b70912e-e3c7-4ca5-8993-656d5a4671b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f74f70f3a911414e8926ebd5d9eaa34b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(my_saved_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccI9LvnkUdh-",
        "outputId": "d60aaa2e-c706-49f1-ecc3-a3eb3d1f23a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PeftModel(\n",
            "  (base_model): LoraModel(\n",
            "    (model): MistralForCausalLM(\n",
            "      (model): MistralModel(\n",
            "        (embed_tokens): Embedding(32768, 4096)\n",
            "        (layers): ModuleList(\n",
            "          (0-31): 32 x MistralDecoderLayer(\n",
            "            (self_attn): MistralSdpaAttention(\n",
            "              (q_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "              (v_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "              (rotary_emb): MistralRotaryEmbedding()\n",
            "            )\n",
            "            (mlp): MistralMLP(\n",
            "              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "              (act_fn): SiLU()\n",
            "            )\n",
            "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JV5pWJcHJCOA"
      },
      "outputs": [],
      "source": [
        "#Run Inference\n",
        "# Step 1: Load the fine-tuned model and tokenizer\n",
        "#model_dir = \"/content/drive/MyDrive/mistral-lora-finetuned\"  # Path to your fine-tuned model\n",
        "#tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "#model = AutoModelForCausalLM.from_pretrained(model_dir)\n",
        "\n",
        "\n",
        "# Step 3: Define a function for inference\n",
        "def generate_text(prompt, max_length=128, temperature=0.1, top_p=0.1):\n",
        "    # Tokenize the input prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate text with the fine-tuned model\n",
        "    with torch.no_grad():\n",
        "        output = my_saved_model.generate(**inputs,\n",
        "                                max_length=max_length,\n",
        "                                pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode the generated tokens back into text\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    return generated_text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_saved_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haRckKgpc0_W",
        "outputId": "0e39fb56-4bd7-458f-cce0-ec04952e2f93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModel(\n",
              "  (base_model): LoraModel(\n",
              "    (model): MistralForCausalLM(\n",
              "      (model): MistralModel(\n",
              "        (embed_tokens): Embedding(32768, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x MistralDecoderLayer(\n",
              "            (self_attn): MistralSdpaAttention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "              (rotary_emb): MistralRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): MistralMLP(\n",
              "              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"In the non-disclosure agreement between Creative Labs and Pacific Magtron how long have the parties have agreed to not share information\"\n",
        "generated_text = generate_text(prompt, max_length=256)"
      ],
      "metadata": {
        "id": "VKcmwlGCViAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjOSywNCXm3v",
        "outputId": "1498cd7c-5ca1-4be0-b53f-183c457985e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the non-disclosure agreement between Creative Labs and Pacific Magtron how long have the parties have agreed to not share information with each other?\n",
            "\n",
            "The agreement is for 10 years.\n",
            "\n",
            "What is the penalty for breaching the agreement?\n",
            "\n",
            "The penalty is $100,000.\n",
            "\n",
            "What is the penalty for breaching the agreement?\n",
            "\n",
            "The penalty is $100,000.\n",
            "\n",
            "What is the penalty for breaching the agreement?\n",
            "\n",
            "The penalty is $100,000.\n",
            "\n",
            "What is the penalty for breaching the agreement?\n",
            "\n",
            "The penalty is $100,000.\n",
            "\n",
            "What is the penalty for breaching the agreement?\n",
            "\n",
            "The penalty is $100,000.\n",
            "\n",
            "What is the penalty for breaching the agreement?\n",
            "\n",
            "The penalty is $100,000.\n",
            "\n",
            "What is the penalty for breaching the agreement?\n",
            "\n",
            "The penalty is $100,000.\n",
            "\n",
            "What is the penalty for breaching the agreement?\n",
            "\n",
            "The penalty is $100,000.\n",
            "\n",
            "What is the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDWUT72fMFU_"
      },
      "outputs": [],
      "source": [
        "#Qualitative Model Testing\n",
        "#Qquestion 1\n",
        "question1_prompt = \"In a typical Hitachi Corporation Non-disclosure agreement, what is the length of time the exiting employee cannot disclose trade secrets?\"\n",
        "Correct_response = \"Exiting employee is bound to NDA for 6 months\"\n",
        "Testing_outcome = model(question1_prompt)\n",
        "\n",
        "my_question1_metric = magic_formula_for_accuracy(correct_response, testing_outcome)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1b6efa5f487f44bc9af2405c33157db8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f80285b34b8401ca6ae3724107e4e9a",
            "placeholder": "​",
            "style": "IPY_MODEL_b26733244f7a4e7db205d43bd5e871a1",
            "value": " 3/3 [00:05&lt;00:00,  1.73s/it]"
          }
        },
        "1c74f736ee914ed29419618bd54791ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1f80285b34b8401ca6ae3724107e4e9a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25b7ee0cdee84e6c9952f7cf0c63dd27": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_afc6720498714f5aa5675e24956ace69",
              "IPY_MODEL_f0c0594f039243999e2caf1a5cbf096e",
              "IPY_MODEL_1b6efa5f487f44bc9af2405c33157db8"
            ],
            "layout": "IPY_MODEL_986d32c09d254912b7d3c8a039273284"
          }
        },
        "7dc0af7657e44005b6c96e8651924096": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ceafc43f0544ba09c02f39c5ed93ba4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "986d32c09d254912b7d3c8a039273284": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a19c03e0988b480590bdeaa2c8e592f4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afc6720498714f5aa5675e24956ace69": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a19c03e0988b480590bdeaa2c8e592f4",
            "placeholder": "​",
            "style": "IPY_MODEL_7dc0af7657e44005b6c96e8651924096",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "b26733244f7a4e7db205d43bd5e871a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0c0594f039243999e2caf1a5cbf096e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ceafc43f0544ba09c02f39c5ed93ba4",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c74f736ee914ed29419618bd54791ac",
            "value": 3
          }
        },
        "f74f70f3a911414e8926ebd5d9eaa34b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_68405c4024de40db9e729a760814dd2b",
              "IPY_MODEL_86905d641d9f4315a2d9725180e2f5d4",
              "IPY_MODEL_a90e5f7295e64a69990db450a91d091d"
            ],
            "layout": "IPY_MODEL_0362c807eeb04e24a939e64137493a7a"
          }
        },
        "68405c4024de40db9e729a760814dd2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7d75989b71e4e0fb2162a118209ef6a",
            "placeholder": "​",
            "style": "IPY_MODEL_595c1ff947e9424cb6036c84148eced7",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "86905d641d9f4315a2d9725180e2f5d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11518c1de3d9427aa75bd70f7eadccb6",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a45d6423c7b743f39eb993597add1aeb",
            "value": 3
          }
        },
        "a90e5f7295e64a69990db450a91d091d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af27f7f95cc7414dac097daad095377b",
            "placeholder": "​",
            "style": "IPY_MODEL_5621228c360143368343e2ded7219d40",
            "value": " 3/3 [00:07&lt;00:00,  2.29s/it]"
          }
        },
        "0362c807eeb04e24a939e64137493a7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7d75989b71e4e0fb2162a118209ef6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "595c1ff947e9424cb6036c84148eced7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11518c1de3d9427aa75bd70f7eadccb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a45d6423c7b743f39eb993597add1aeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "af27f7f95cc7414dac097daad095377b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5621228c360143368343e2ded7219d40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}